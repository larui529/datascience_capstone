{
    "collab_server" : "",
    "contents" : "require(digest)\nrequire(stringi)\nrequire(data.table)\n\n################################################################################################\n#\n# 01. Loading of benchmark data sets\n#\n################################################################################################\n\n\n# 01b. Get text from randomly selected tweets\n################################################################################################\n\ntweets <- readLines('data/tweets.txt', encoding = 'UTF-8')\n\n# verify checksum of loaded lines\ndigest(paste0(tweets, collapse = '||'), \n       algo='sha256', \n       serialize=F)==\n    \"7fa3bf921c393fe7009bc60971b2bb8396414e7602bb4f409bed78c7192c30f4\"\n\n\n# 01c. Get text from randomly selected blog descriptions\n################################################################################################\n\n# make sure we can read it back in\nblogs <- readLines('data/blogs.txt', encoding = 'UTF-8')\n\n# verify checksum of loaded lines\ndigest(paste0(blogs, collapse = '||'), \n       algo='sha256', \n       serialize=F)==\n    \"14b3c593e543eb8b2932cf00b646ed653e336897a03c82098b725e6e1f9b7aa2\"\n\n\n\n################################################################################################\n#\n# 02. Define the functions used for benchmarking\n#\n################################################################################################\n\n# 02a. Pre-processing functions\n################################################################################################\n\n# split.sentence\n#  Returns a matrix containing in column i the part of the line before the ith word (sentence) \n#  and the ith word (nextWord).\n#  The function is used in benchmark to generate and evaluate predictions for the partial lines.\nsplit.sentence <- compiler::cmpfun(function(line) {\n    require(stringi)\n    # append a space to the sentence (to make sure we always create one result with only the \n    # last word missing)\n    sent <- paste0(line, ' ')\n\n    sep <- stri_locate_all_regex(line, \n                                 pattern = '[^\\\\w\\'@#\\u2018\\u2019\\u201b]+', \n                                 omit_empty=T, \n                                 case_insensitive=T)[[1]]\n    sapply(seq_len(nrow(sep)), \n           function(i) {\n               c(sentence=ifelse(i>1, substr(line, 1, sep[i-1,2]), ''), \n                    nextWord=tolower(substr(line, max(sep[i-1,2]+1, 1), min(nchar(line), sep[i,1]-1)))\n               )\n               })\n}, options=list(optimize=3))\n\n\n# 02b. Benchmarking function\n################################################################################################\n\n# benchmark\n#  Evaluates the performance of a next word prediction algorithm based on the provided test data-\n#  set(s).\n#\n#  Parameters\n#   FUN         Function that produces the next word prediction. The function should take a single \n#               character value as first input and return a vector of character values represen-\n#               ting the top-3 predictions (with the 1st value being the first prediction).\n#   ...         Additional parameters to pass to FUN.\n#   sent.list   Named list of character vectors containing the text lines used for the benchmark.\n#   ext.output  If TRUE, return additional details about the R environment and loaded packages \n#               after completing the benchmark.\nbenchmark <- compiler::cmpfun(function(FUN, ..., sent.list, ext.output=T) {\n    require(stringi)\n    require(digest)\n    require(data.table)\n    \n    result <- rbindlist(lapply(names(sent.list), \n           function(list.name) {  \n               sentences <- sent.list[[list.name]]\n               \n               score <- 0\n               max.score <-0\n               hit.count.top3 <- 0\n               hit.count.top1 <- 0\n               total.count <- 0\n               time <- system.time({\n                   for (sent in sentences) {\n                       split <- split.sentence(sent[1])\n                       max.score <- max.score + ncol(split)*3\n                       total.count <- total.count + ncol(split)\n                       rank <- sapply(seq_len(ncol(split)),\n                                      function(i) {\n                                          min(which(FUN(split[1,i], ...)==split[2,i]),4)\n                                      })\n                       score <- score + sum(4-rank)\n                       hit.count.top3 <- hit.count.top3 + sum(rank<4)\n                       hit.count.top1 <- hit.count.top1 + sum(rank==1)\n                   }\n               })\n               \n               list('list.name' = list.name,\n                    'line.count' = length(sentences),\n                    'word.count' = sum(stri_count_words(sentences)),\n                    'hash' = digest(paste0(sentences, collapse = '||'), algo='sha256', serialize=F),\n                    'score' = score,\n                    'max.score' = max.score,\n                    'hit.count.top3' = hit.count.top3,\n                    'hit.count.top1' = hit.count.top1,\n                    'total.count' = total.count,\n                    'total.runtime' = time[3]\n               )               \n           }), use.names=T)\n    \n    setkey(result, list.name)\n    \n    # The overall scores are calculated weighting each data set equally (independent of the \n    # number of lines in each dataset).\n    overall.score.percent = 100 * result[,sum(score/max.score)/.N]\n    overall.precision.top3 = 100 * result[,sum(hit.count.top3/total.count)/.N]\n    overall.precision.top1 = 100 * result[,sum(hit.count.top1/total.count)/.N]\n    average.runtime = 1000 * result[,sum(total.runtime)/sum(total.count)]\n    number.of.predictions = result[,sum(total.count)]\n    total.mem.used = sum(unlist(lapply(ls(.GlobalEnv),\n                                       function(x) {\n                                           object.size(get(x,\n                                                           envir = .GlobalEnv,\n                                                           inherits = FALSE))\n                                           })))/(1024^2)\n    cat(sprintf(paste0('Overall top-3 score:     %.2f %%\\n',\n                       'Overall top-1 precision: %.2f %%\\n',\n                       'Overall top-3 precision: %.2f %%\\n',\n                       'Average runtime:         %.2f msec\\n',\n                       'Number of predictions:   %d\\n',\n                       'Total memory used:       %.2f MB\\n'),\n                overall.score.percent,\n                overall.precision.top1,\n                overall.precision.top3,\n                average.runtime,\n                number.of.predictions,\n                total.mem.used\n                ))\n    \n    cat('\\nDataset details\\n')\n    for (p.list.name in result$list.name) {\n        res <- result[list(p.list.name)]\n        cat(sprintf(paste0(' Dataset \"%s\" (%d lines, %d words, hash %s)\\n',\n                           '  Score: %.2f %%, Top-1 precision: %.2f %%, Top-3 precision: %.2f %%\\n'\n                           ),\n                    p.list.name,\n                    res$line.count,\n                    res$word.count,\n                    res$hash,\n                    100 * res$score/res$max.score,\n                    100 * res$hit.count.top1/res$total.count,\n                    100 * res$hit.count.top3/res$total.count\n        ))\n    }\n    \n    if (ext.output==T) {\n        packages <- sort(stri_replace_first_fixed(search()[stri_detect_regex(search(), \n                                                                             '^package:')], \n                                                  'package:', ''))\n        \n        cat(sprintf(paste0('\\n\\n%s, platform %s\\n', \n                           'Attached non-base packages:   %s\\n',\n                           'Unattached non-base packages: %s'\n                           ),\n                   sessionInfo()$R.version$version.string,\n                   sessionInfo()$platform,\n                   paste0(sapply(sessionInfo()$otherPkgs, \n                                 function(pkg) {\n                                     paste0(pkg$Package, ' (v', pkg$Version, ')')\n                                 }), \n                          collapse = ', '),\n                   paste0(sapply(sessionInfo()$loadedOnly, \n                                 function(pkg) { \n                                     paste0(pkg$Package, ' (v', pkg$Version, ')')\n                                 }), \n                          collapse = ', ')\n                   ))\n    }\n}, options=list(optimize =3))\n\n\n\n\n################################################################################################\n#\n# 03. Define the wrapper function to be called by benchmark\n#\n################################################################################################\n\n# As an example, we create a very simple baseline algorithm which always returns\n# the three most frequent English words.\npredict.baseline <- function(x){c('the', 'on', 'a')}\n\n\n\n################################################################################################\n#\n# 04. Perform the benchmark\n#\n################################################################################################\nbenchmark(predict.baseline, \n          # additional parameters to be passed to the prediction function can be inserted here\n          sent.list = list('tweets' = tweets, \n                           'blogs' = blogs), \n          ext.output = T)\n",
    "created" : 1480374328945.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1036582278",
    "id" : "A03CC326",
    "lastKnownWriteTime" : 1480346136,
    "last_content_update" : 1480346136,
    "path" : "C:/Users/stbal/Desktop/benchmark/benchmark.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}