{
    "collab_server" : "",
    "contents" : "\n# load required libraries -------------------------------------------------\n\nsuppressMessages(c(library(ANLP),\n                 library(dplyr),\n                 library(tidytext),\n                 library(doParallel),\n                 library(tidyverse)))\n#--------------------------------------------------------------------------\n\n\n\n\n# read-in data and create a sample data -----------------------------------\n\nblogs <- readTextFile(\"final/en_US/en_US.blogs.txt\", encoding = \"UTF-8\")\nnews <- readTextFile(\"final/en_US/en_US.news.txt\", encoding = \"UTF-8\")\ntwitter <- readTextFile(\"final/en_US/en_US.twitter.txt\", encoding = \"UTF-8\")\n\nmerged_data <- c(blogs, news, twitter)\nlength(merged_data)\n\nset.seed(1234)\nsampled_data <- sampleTextData(merged_data, 0.1)\nlength(sampled_data)\n\nrm(blogs, news, twitter)\n#--------------------------------------------------------------------------\n\n\n\n\n\n# clean and filter profane words ------------------------------------------\n\n## my function for profane words ##\nmy_stopwords <- function() {\n  swear <- readLines(\"swearWords.txt\", encoding = \"UTF-8\")\n  bad <- readLines(\"bad-words.txt\", encoding = \"UTF_8\")\n  no_meaning <- readLines(\"no_meaning.txt\")\n  c(swear, bad, no_meaning)\n}\n\ncleaned_data <- sampled_data %>% \n  removeWords(my_stopwords()) %>%\n  cleanTextData()\n#--------------------------------------------------------------------------\n\n\n\n\n# convert the corpus back to a dataframe ----------------------------------\n\ntidied_corpus <- cleaned_data %>%\n  tidy() %>%\n  select(text)\n#-------------------------------------------------------------------------\n\n\n\n\n# generate tokens ---------------------------------------------------------\n\nremove_function <- function(doc, n = 3) {\n  dplyr::filter(doc, freq >= n)\n}\n\n\ngenerate_tokens <- function(doc, token = 1, remove = 3) {\n  doc %>%\n    unnest_tokens(word, text, token = \"ngrams\", n = token) %>% \n    count(word) %>%\n    rename(freq = n) %>%\n    filter(freq >= remove) %>%\n    arrange(desc(freq))\n}\n\nunigram <- generate_tokens(tidied_corpus, token = 1)\n\nbigram <- generate_tokens(tidied_corpus, token = 2)\n\ntrigram <- generate_tokens(tidied_corpus, token = 3, remove = 2)\n\ntetragram <- generate_tokens(tidied_corpus, token = 4, remove = 2)\n\nquintugram <- generate_tokens(tidied_corpus, token = 5, remove = 2)\n#--------------------------------------------------------------------------\n\n\n\n\n# create test and train datasets ------------------------------------------\n\n## function to create the training datasets\nrows <- sample(nrow(data), ceiling(nrow(data) * 0.9)) ## define \"rows\" to be used\n# by the functions\n\ntrain_gram <- function(data) {\n  as.data.frame(data[rows, ])\n}\n\n## function to create the testing dataset ##\ntest_gram <- function(data) {\n  as.data.frame(data[- rows, ])\n}\n\n# splitting the ngrams into train and test datasets\nunigram_train <- train_gram(unigram)\nbigram_train <- train_gram(bigram)\ntrigram_train <- train_gram(trigram)\ntetragram_train <- train_gram(tetragram)\nquintugram_train <- train_gram(quintugram)\n\nunigram_test <- test_gram(unigram)\nbigram_test <- test_gram(bigram)\ntrigram_test <- test_gram(trigram)\ntetragram_test <- test_gram(tetragram)\nquintugram_test <- test_gram(quintugram)\n#--------------------------------------------------------------------------\n\n\n\n\n# prediction model ---------------------------------------------------------\n\nngram_models <- list(quintugram_train, \n                     tetragram_train, \n                     trigram_train, \n                     bigram_train, \n                     unigram_train)\n",
    "created" : 1480318748327.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2660683798",
    "id" : "EAD4C528",
    "lastKnownWriteTime" : 1480456898,
    "last_content_update" : 1480456899256,
    "path" : "C:/Users/stbal/Dropbox/coursera2/jhbsph/capstone/project/data_science_capstone/prediction.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}