{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Data Science Capstone Project Milestone Report\"\nauthor: \"Balogun Stephen Taiye\"\ndate: \"November 22, 2016\"\noutput:\n  html_document:\n    code_download: no\n    css: style.css\n    fig_caption: yes\n    theme: flatly\n    highlight: tango\n    toc: yes\n    toc_float: yes\n    fig.width: 8\n    fig.height: 6\n    code_folding: show\n  html_notebook:\n    toc: yes\n    toc_float: TRUE\n    css: style.css\n    fig.width: 8\n    fig.height: 6\n---\n\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(message = FALSE, warning = FALSE,\n                      collapse = TRUE)\nlibrary(ggplot2)\ntheme_set(theme_light())\n```\n\n\n# Introduction\n\nThis is the John Hopkins Bloomberg School of Public Health Coursera Data Science\nmilestone report. The goal of this project is to: \n\n -  Demonstrate that the data hasb been successfully downloaded \n \n -  Create a basic report of summary statistics about the data sets  \n \n -  Report any interesting findings that you amassed so far \n \n -  Get feedback on your plans for creating a prediction algorithm and Shiny app. \n\nThis capstone project is a collaborative effort of the John Hopkins and the \nSwiftKey teams.\n\n\n# Downloading the file\n\nThe documents supplied for this capstone project is rather heavy consisting of \nseveral million lines of test. The size of the zipped folder is approximately 0.5GB. \n\n\n```{r, \"downloading\"}\nlibrary(downloader)\n\nfileURL <- \"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/\nCoursera-SwiftKey.zip\"\n\nif (!file.exists(\"Coursera-SwiftKey.zip\")) {\n  download(fileURL)\n  unzip(\"Coursera-SwiftKey.zip\")\n}\n\n```\n\n\n```{r, \"read-in data\"}\n## opening of connection done to link to each of the documents and closing of each connection ensured before a another is opened\n\ncon1 <- file(description = \"final/en_US/en_US.blogs.txt\", open = \"r\")\nblogs <- readLines(con1, encoding = \"UTF-8\")  \nclose(con1)\n\ncon2 <- file(description = \"final/en_US/en_US.news.txt\", open = \"r\")\nset.seed(1235)\nnews <- readLines(con2, encoding = \"UTF-8\")\nclose(con2)\n\ncon3 <- file(description = \"final/en_US/en_US.twitter.txt\", open = \"r\")\n\nset.seed(1236)\ntwitter <- readLines(con3, encoding = \"UTF-8\") \nclose(con3)\n```\n\n\n\n```{r, \"loading required packages\", include = FALSE}\n## load required libraries\n\nsuppressMessages(c(library(tidyverse),\n                   library(doParallel),\n                   library(stringr),\n                   library(qdap),\n                   library(tm)))\n```\n\n\n\n```{r, \"loading packages\", eval = FALSE}\nsuppressMessages(c(library(tidyverse),\n                   library(doParallel),\n                   library(stringr),\n                   library(qdap),\n                   library(tm)))\n```\n\n\nThe table below shows the summary of the downloaded file.\n\n```{r, \"summary\", cache = TRUE}\n\nlines <- c(length(news), length(blogs), length(twitter)) ## counts the number of lines per files\ncharacter <- c(sum(str_length(news)), \n               sum(str_length(blogs)), \n               sum(str_length(twitter)))  ## counts the number of characters in each of the files\nwords <- c(sum(str_count(news, boundary(\"word\"))), \n          sum(str_count(blogs, boundary(\"word\"))), \n           sum(str_count(twitter, boundary(\"word\"))))  ## counts the number of words in each of the file. \n\nsumm <- rbind(lines, words, character)  \nsumm <- as.data.frame(summ)  ## convert the binded lines into a dataframe\nnames(summ) <- c(\"news\", \"blogs\", \"twitter\") ## add column names to the dataframe\nrow.names(summ) <- c(\"lines\", \"words\", \"characters\") ## add row names\nsumm <- format(summ, big.mark = \",\")  ## format the data nicely\n```\n\n```{r, \"kable summ\", results = \"asis\"}\nknitr::kable(summ, caption = \"table 1: summary of 'lines', 'words' and 'characters' in the project file\")  ## for nice display\n```\n\n\nAfter the file was downloaded, surfing through, I decided to do my analysis based on a 10% sample size of the document. \nThis makes working with the data relatively more easy while increasing the speed \nof data processing and reducing the chances of my computer getting clogged in \nthe process. \n\nThe sampled data were written into another directory (\"sample_text\") to make creation of document corpus easy\n\n\n```{r, \"sampling\"}\n\nset.seed(1234)  ## for reproducibility \ntwitter_ten <- sample(twitter, (1/10 * length(twitter)))\n\nset.seed(1234)\nblogs_ten <- sample(blogs, (1/10 * length(blogs)))\n\nset.seed(1234)\nnews_ten <- sample(news, (1/10 * length(news)))\n\n```\n\n\n```{r, \"write sample into folder\"}\nif (!file.exists(\"sample_text\")) {\n  dir.create(\"sample_text\")\n  writeLines(twitter_ten, \"sample_text/twitter_ten\")\n\n  writeLines(news_ten, \"sample_text/news_ten\")\n\n  writeLines(blogs_ten, \"sample_text/blogs_ten\")\n}\n\n```\n\n\n# Corpus creation\n\nUsing the `tm` package, a corpus of document was created using the sampled files. \nCorpus creation allows us to do parallel processing on multiple text files.\n\n\n```{r, \"read corpus\"}\nfile_directory <- file.path(\".\", \"sample_text\")\nfile_directory\ndir(file_directory)\ndocs <- VCorpus(DirSource(file_directory, encoding = \"UTF-8\"), \n                readerControl = list(language = \"en\"))  ## using the \"UTF-8\" \n## encoding ensures that symbols, signs and other characters are correctly read\n```\n\n\n```{r, \"exploring the corpus\"}\nsummary(docs)  ## displays the summary of the whole corpus formed\n\nsapply(docs[1:3], meta)  ## displays the meta-data of each text document\n\n## function to view the top 6 lines of each of the documents\nview_doc <- function(x, n = 6)  {\n  output <- vector(\"character\")\n  for (i in seq_along(x)) {            # 2. sequence\n    output[i] <- list(head(as.character(x[[i]]), n))  # 3. body\n  }\n  output\n}\n\nview_doc(docs, 3)  ## views the first three lines for each of the files\n```\n\n\n# Text processing\n\nFollowing exploration of the documents, it became obvious that the document needed some processing. This include:\n\n- conversion of all words to lower characters  \n\n- removal of numbers from the documents (since we are only interested in **words**)\n\n- replace abbreviated words with their full forms (e.g. Mr is converted to Mister)\n\n- replace symbols with their word-equivalent (e.g. \"&\" with \"and\")\n\n- replace contractions (e.g. \"wasn't\" is replaced by \"was not\")\n\n- remove punctuation marks from the documents\n\n- remove any extra white (tab) spaces from the document\n\n- *profanity filtering* (i.e. removal of some words that are agreed as profane), and also removal of some other words that do not appear to make any sense. The profane words were of [swear words](http://www.bannedwordlist.com/lists/swearWords.txt) and [bad words]()\n\n```{r, \"preprocessing\", cache = TRUE}\nmy_stopwords <- function() {\n  swear <- readLines(\"swearWords.txt\", encoding = \"UTF-8\")\n  bad <- readLines(\"bad-words.txt\", encoding = \"UTF-8\")\n  no_meaning <- readLines(\"no_meaning.txt\")\n  \n  c(swear, bad, no_meaning)\n}\n\npreprocess <- function(doc) {\n  doc %>%\n    tm_map(content_transformer(str_to_lower)) %>%\n    tm_map(content_transformer(replace_symbol)) %>%\n    tm_map(content_transformer(replace_abbreviation)) %>%\n    tm_map(content_transformer(replace_contraction)) %>%\n    tm_map(removePunctuation) %>% \n    tm_map(removeNumbers) %>% \n    tm_map(stripWhitespace) %>%\n    tm_map(removeWords, my_stopwords())\n}\n\nprep_doc <- preprocess(docs)\n\nview_doc(prep_doc, 3)\n```\n\nIn the spirit of **tidy data**, I decided to write my *preprocessed corpus* back to their corresponding text files to allow me use **tidy data** principles subsequently on the data.\n\n```{r, \"write corpus\"}\nif (!file.exists(\"processed_doc\")) {\n  dir.create(\"processed_doc\")\n  writeCorpus(prep_doc, \"./processed_doc\")\n}\n```\n\n\n```{r, \"read files\"}\ncon1 <- file(description = \"./processed_doc/twitter_ten.txt\", open = \"r\")\ntwitter <- readLines(con1)  \nclose(con1)\n\ncon2 <- file(description = \"processed_doc/blogs_ten.txt\", open = \"r\")\nblogs <- readLines(con2)\nclose(con2)\n\ncon3 <- file(description = \"processed_doc/news_ten.txt\", open = \"r\")\nnews <- readLines(con3) \nclose(con3)\n```\n\n \n \n```{r, \"dataframe\"}\n\n## convert the character texts into dataframe\n\ntwitter <- data_frame(source = \"twitter\", \n                        line = 1:length(twitter), \n                        text = twitter)\nblogs <- data_frame(source = \"blogs\", \n                    line = 1:length(blogs), \n                    text = blogs)\nnews <- data_frame(source = \"news\", \n                   line = 1:length(news), \n                   text = news)\n\nprep_doc <- bind_rows(twitter, blogs, news)\n```\n\n\n```{r, \"complete preprocessing\"}\n\n## following initial preprocessing, some compound-words and abbreviated words needed to be separated\n\nto_strings <- function(x) {\n  x %>%\n    str_replace(\"teamifollowback\", \"team i follow back\") %>%\n    str_replace(\"sweetiepiez\", \"sweet pie\") %>%\n    str_replace(\"jonathantaylorthomas\", \"jonathan taylor thomas\") %>%\n    str_replace(\"thru\", \"through\") %>%\n    str_replace(\"tvmovies\", \"television movies\") %>%\n    str_replace(\"tv\", \"television\") %>%\n    str_replace(\"shortlived\", \"short lived\")\n}\n\nprep_doc$text <- to_strings(prep_doc$text)\n```\n\n\n# Tokenization\n\nTokenization involves spliting of text files into what we call **tokens**. Tokens are words or group of words derived from spliting up the inital text file. The number of words constituting a token is represented by **_n-gram_** where a **1-gram** tokenization is called a **unigram**; a **2-gram** tokenization called a **bigram** and so on... Tokens are created for $n = 1$ to $n = 4$.\n\nFor this particular task, I am using the `tidytext` package. This works using the principle of tidy data on data_frame and works well with other packages in the `tidyverse`.\n\n```{r,\"tokenization\", cache = TRUE}\nsuppressMessages(library(tidytext))  ## text mining using tidy data principles\n\ntokenize_data <- function(data) {\n  \n  ## create a unigram token\n  token1 <- data %>%\n  unnest_tokens(word, text, token = \"ngrams\", n = 1) %>% \n  mutate(word = str_extract(word, \"[^\\\\d]+\")) %>%\n  drop_na() %>%\n  ungroup\ntoken1$gram <- \"unigram\"\n\n## create a bigram token\ntoken2 <- data %>%\n  unnest_tokens(word, text, token = \"ngrams\", n = 2) %>% \n  mutate(word = str_extract(word, \"[^\\\\d]+\")) %>%\n  drop_na() %>%\n  ungroup\ntoken2$gram <- \"bigram\"\n\n## create a trigram token\ntoken3 <- data %>%\n  unnest_tokens(word, text, token = \"ngrams\", n = 3) %>% \n  mutate(word = str_extract(word, \"[^\\\\d]+\")) %>%\n  drop_na() %>%\n  ungroup\ntoken3$gram <- \"trigram\"\n\n## create a tetragram token\ntoken4 <- data %>%\n  unnest_tokens(word, text, token = \"ngrams\", n = 4) %>% \n  mutate(word = str_extract(word, \"[^\\\\d]+\")) %>%\n  drop_na() %>%\n  ungroup\ntoken4$gram <- \"tetragram\"\n\nbind_rows(token1, token2, token3, token4)\n}\n\n\ntoken_doc <- tokenize_data(prep_doc)\n```\n\n\n```{r, \"table 2\"}\nknitr::kable(head(token_doc, 4), caption = \"table 2: first 4 rows of the tokenized data\")\n\n```\n\n\n\n# Analysis {.tabset .tabset-fade .tabset-pills}\n\nFollowing **tokenization**, we will take a look to find out the most common tokens in each of the *n-grams*. We will be looking at the 15 most common words for each of the tokens\n\n```{r, \"word count\", cache = TRUE}\nword_count <- function(document) {\n  document %>%\n    group_by(source, gram, word) %>%\n    count(sort = TRUE) %>% ## counts the number of times a particular word appears in each of the source for each of the grams\n    rename(count = n)\n}\n\nword_doc <- word_count(token_doc)\n\n```\n\n\n\n\n```{r, \"plot top 15 words\"}\ntop_15 <- function(x) {\n  x %>% \n    group_by(source, gram) %>% \n    top_n(15) %>%\n    mutate(word = reorder(word, count)) %>%\n    arrange(source)\n  }\n\ntop15_words <- top_15(word_doc)\n```\n\n## blogs \n\n```{r, \"blogs plot\", fig.cap = \"figure 1: top 15 tokens in the 'blogs' text file\"}\nplot_blogs <- top15_words %>%\n    filter(source == \"blogs\") %>%\n  ggplot(aes(word, count, fill = gram)) + \n  geom_bar(alpha = 0.8, stat = \"identity\", show.legend = FALSE) + \n  facet_wrap(~ gram, ncol = 2, scales = \"free\") + \n  coord_flip() + \n  labs(title = \"top-15 tokens in the sampled blogs data\", \n       caption = \"10% of 'blogs' data\",\n       x = NULL,\n       y = \"count\")\n  \n\nplot_blogs\n```\n\n\n## news\n\n```{r, \"news plot\", fig.cap = \"figure 2: top 15 tokens in the 'news' text file\"}\nplot_news <- top15_words %>%\n  filter(source == \"news\") %>%\n  ggplot(aes(word, count, fill = gram)) + \n  geom_bar(alpha = 0.8, stat = \"identity\", show.legend = FALSE) + \n  facet_wrap(~ gram, ncol = 2, scales = \"free\") + \n  coord_flip() +\n  labs(title = \"top-15 tokens in the sampled news data\",\n       caption = \"10% of 'news' data\",\n       x = NULL,\n       y = \"count\")\n\nplot_news\n```\n\n\n## twitter\n\n```{r, \"twitter plot\", fig.cap = \"figure 3: top 15 tokens in the 'twitter' text file\"}\nplot_twitter <- top15_words %>%\n  filter(source == \"twitter\") %>%\n  ggplot(aes(word, count, fill = gram)) + \n  geom_bar(alpha = 0.8, stat = \"identity\", show.legend = FALSE) + \n  facet_wrap(~ gram, ncol = 2, scales = \"free\") +\n  coord_flip() +\n  labs(title = \"top-15 tokens in the sampled twitter data\", \n       caption = \"10% of 'twitter' data\",\n       x = NULL,\n       y = \"count\")\n\nplot_twitter\n```\n\n\n# Summary of sampled data\n\nThe table below shows the *number of* **tokens**  in each of the sampled files, the *number of unique* **token** per file, and the *ratio of* of this to each other. \n\nUnigrams obviously have higher ratio with the value decreasing as *n* increases. The table also shows that the values are generally lowest for **news** data and highest for **blogs** data. This indicates a high ratio of vocabulary use in the **news** data compared to the others.\n\n```{r, \"summary chunk\", cache = TRUE}\n# count unique words per sample size \nunique_token <- token_doc %>%\n  group_by(source, gram) %>%\n  summarise(unique = length(levels(factor(word))))\n\n# summarise the number of words per document\ntotal <- word_doc %>%\n  group_by(source, gram) %>%\n  summarise(total_words = sum(count))\n\n\n# create a summary table \nsummary_tab <- left_join(total, unique_token, by = c(\"source\", \"gram\"))\n\nsummary_tab$gram <- factor(summary_tab$gram, \n                           levels = c(\"unigram\", \"bigram\", \"trigram\", \"tetragram\"))\n\nsummary_tab <- summary_tab %>%\n  mutate(ratio = total_words / unique) %>%\n  arrange(source, gram) %>%\n  select(source, gram, unique, total_words, ratio) %>%\n  format(digits = 2, nsmall = 2)\n```\n\n```{r, \"summary tab\", fig.cap = \"table 3: summary statistics of the tokenized data\", results = \"asis\"}\nDT::datatable(summary_tab)\n```\n\n\n# Plans for creating the prediction algorithm and Shiny app\n\nI plan on building the prediction algorithm using the **tetragram** tokenization (i.e. $n =  4$). This should enable the generation of a predicted word after three words has been typed by the user. This algorithm will be built into the shinyapp when it is finally created.\n\n# Challenges\n\nI am still having challenges figuring out the packages to use for the building the prediction model for text files. I will definitely appreciate any suggestion as regards this.\n\nThanks.",
    "created" : 1480325716183.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "121994440",
    "id" : "D335C144",
    "lastKnownWriteTime" : 1480200264,
    "last_content_update" : 1480200264,
    "path" : "C:/Users/stbal/Dropbox/coursera2/jhbsph/capstone/project/data_science_capstone/milestone_report/milestone_report.Rmd",
    "project_path" : "milestone_report.Rmd",
    "properties" : {
        "last_setup_crc32" : "2A3B82515841c627"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}